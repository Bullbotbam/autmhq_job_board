{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bullbotbam/autmhq_job_board/blob/main/automated_job_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium"
      ],
      "metadata": {
        "id": "Kq2SCFEAu8f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1f1152-8fb0-4caf-f759-b64df57cdab5"
      },
      "id": "Kq2SCFEAu8f2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.5.18.1)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 34.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.2.0)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.2.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d6668d-772f-4e74-ab4f-7c10310f3261",
      "metadata": {
        "id": "b7d6668d-772f-4e74-ab4f-7c10310f3261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f313d41b-3e38-4931-a8b2-a467a6e2ef7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver \n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait \n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.common.exceptions import NoSuchElementException,TimeoutException,StaleElementReferenceException\n",
        "\n",
        "import time\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re \n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86ac7ed-298b-46a9-bbf0-fec210fad5c3",
      "metadata": {
        "id": "c86ac7ed-298b-46a9-bbf0-fec210fad5c3"
      },
      "outputs": [],
      "source": [
        "def make_noise():\n",
        "    sys.stdout.write('\\a')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "## Helper functions\n",
        "\n",
        "### Formatting the date:\n",
        "def split_date(x):\n",
        "    m = re.search(r'\\d+[+]? \\w+ \\w+' ,str(x))\n",
        "    if m:\n",
        "        return m.group()\n",
        "    else:\n",
        "        return \" \"\n",
        "    \n",
        "def convert_date(x):\n",
        "    m = re.search(r'\\d+' ,str(x))\n",
        "    if m:\n",
        "        dt = datetime.today()-timedelta(int(m.group()))\n",
        "        dt = dt.strftime('%m/%d/%Y')  #time_dict)\n",
        "        return dt\n",
        "    else:\n",
        "        return \" \"\n",
        "    \n",
        "### Finding Zip and City\n",
        "def find_zip(x):\n",
        "    z = re.search(r'\\d+',x )\n",
        "    if z:\n",
        "         return(z.group())\n",
        "    else :\n",
        "        return \" \""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edc64729-8e94-4aaa-89d2-0f5a033d14e9",
      "metadata": {
        "id": "edc64729-8e94-4aaa-89d2-0f5a033d14e9"
      },
      "source": [
        "## Indeed "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14098afe-b89c-4ca2-8eea-ce9df0963e2f",
      "metadata": {
        "id": "14098afe-b89c-4ca2-8eea-ce9df0963e2f"
      },
      "outputs": [],
      "source": [
        "def scrape_indeed_jobs(keyword, location, n_pages):\n",
        "\n",
        "    url = f\"https://www.indeed.com/?from=gnav-homepage\"\n",
        "    ## Create an instance of the chrome webdriver. This is to interact with the website and test my code\n",
        "    s = Service ('C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')\n",
        "    wd = webdriver.Chrome(service = s)\n",
        "    wait = WebDriverWait(wd, 5)\n",
        "\n",
        "    ## Open Indeed Webpage \n",
        "    wd.get(url)\n",
        "\n",
        "    ## Enter search parameters\n",
        "    what_search = wd.find_element(By.ID, 'text-input-what')\n",
        "    what_search.send_keys(keyword)\n",
        "\n",
        "    where_search = wd.find_element(By.ID, 'text-input-where')\n",
        "    where_search.send_keys(Keys.CONTROL + \"a\")\n",
        "    where_search.send_keys(Keys.DELETE)\n",
        "    where_search.send_keys(location)\n",
        "    where_search.send_keys(Keys.RETURN)\n",
        "\n",
        "    base_url = wd.current_url\n",
        "\n",
        "    ## Initialise the different lists:\n",
        "    job_id =  []\n",
        "    job_title = []\n",
        "    seniority = []\n",
        "    emp_type=[]\n",
        "    job_link = []\n",
        "    industries = []\n",
        "    company_names = []\n",
        "    company_names2 = []\n",
        "    job_date =[]\n",
        "    job_location = []\n",
        "    job_salary = []\n",
        "    job_qual = []\n",
        "    job_description = []\n",
        "    \n",
        "    def extract(wait,job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description):\n",
        "        results = wait.until(\n",
        "                EC.presence_of_element_located((By.ID, \"mosaic-provider-jobcards\")))\n",
        "        jobs = results.find_elements(By.TAG_NAME,\"a\")\n",
        "        for job in jobs:\n",
        "            try:\n",
        "                href = job.get_attribute(\"href\")\n",
        "                job_link.append(href)\n",
        "            except(NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                job_link.append('')\n",
        "\n",
        "            ## Find the job's id\n",
        "            try :\n",
        "                ids = job.get_attribute(\"id\")\n",
        "                job_id.append(ids)\n",
        "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                job_id.append('')\n",
        "\n",
        "            ## Find date posted\n",
        "            try:\n",
        "                date = job.find_element(By.CLASS_NAME,'date').text\n",
        "                # print('date', date)\n",
        "                job_date.append(date)\n",
        "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                job_date.append(\"\")\n",
        "\n",
        "               ## Find the company name alternative :\n",
        "            try:\n",
        "                company2 = job.find_element(By.XPATH,'//a[@data-tn-element = \"companyName\"]').get_attribute(\"textContent\")\n",
        "                company_names2.append(company2)                \n",
        "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                company_names2.append('')\n",
        "\n",
        "            try:\n",
        "\n",
        "                job.find_element(By.CSS_SELECTOR, 'div').click()\n",
        "                time.sleep(2)\n",
        "                iframe = results.find_element(By.ID,\"vjs-container\").find_element(By.CSS_SELECTOR,'iframe')\n",
        "                wd.switch_to.frame(iframe)\n",
        "\n",
        "                        ## Find the job's title \n",
        "                try:\n",
        "                    title = wait.until(EC.presence_of_element_located((By.TAG_NAME,\"h1\"))).get_attribute(\"textContent\")\n",
        "                    title = title.replace(\" - job post\", \"\")\n",
        "                    job_title.append(title)\n",
        "                        #print(title)   \n",
        "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                    job_title.append('')\n",
        "\n",
        "\n",
        "                    ## Find the company name:\n",
        "                try:\n",
        "                    company = wait.until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"viewJobSSRRoot\"]/div[1]/div[1]/div/div/div/div[1]/div/div[1]/div[1]/div[1]/div[2]/div/div/div/div[1]/div[2]/div/a'))).get_attribute('innerText')\n",
        "                        #print(f'company,{company}')//*[@id=\"viewJobSSRRoot\"]/div[1]/div[1]/div/div/div/div[1]/div/div[1]/div[1]/div[2]/div[2]/div/div/div/div[1]/div[2]/div/a\n",
        "                    company_names.append(company)                \n",
        "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                    company_names.append('') \n",
        "\n",
        "                    ## Find the location \n",
        "                try:\n",
        "                    loc = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"viewJobSSRRoot\"]/div[1]/div[1]/div/div/div/div[1]/div/div[1]/div[1]/div[1]/div[2]/div/div/div/div[2]/div'))).get_attribute('innerText')\n",
        "                       # print(f'location, {loc}')                                  //*[@id=\"viewJobSSRRoot\"]/div[1]/div[1]/div/div/div/div[1]/div/div[1]/div[1]/div[2]/div[2]/div/div/div/div[2]/div\n",
        "                    job_location.append(loc)\n",
        "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                    job_location.append('')\n",
        "\n",
        "                    ## Find the salary:\n",
        "                try:\n",
        "                    salary  = wait.until(\n",
        "                        EC.presence_of_element_located((By.XPATH,'//*[@id=\"jobDetailsSection\"]/div[2]/span'))).get_attribute('innerText')\n",
        "                    job_salary.append(salary)\n",
        "                       # print(f'salary,{salary}')\n",
        "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                    job_salary.append('')\n",
        "\n",
        "                    ## Find Employment type: \n",
        "                try:\n",
        "                    emp = wait.until(\n",
        "                        EC.presence_of_element_located((By.XPATH, '//*[@id=\"jobDetailsSection\"]/div[3]/div[2]'))).get_attribute('innerText')             \n",
        "                    emp_type.append(emp)\n",
        "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                    emp_type.append(\"\")\n",
        "\n",
        "                     ## Qualifications            \n",
        "                try:\n",
        "                    qual = wait.until(\n",
        "                        EC.presence_of_element_located((By.CLASS_NAME, \"jobsearch-ReqAndQualSection-item--wrapper\"))).get_attribute('innerText')\n",
        "                       # print('qual',qual)\n",
        "                    job_qual.append(qual)\n",
        "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException):\n",
        "                    job_qual.append('')\n",
        "\n",
        "                    ## Job summary:\n",
        "                try:\n",
        "                    summary = wait.until(\n",
        "                        EC.presence_of_element_located((By.XPATH,'//*[@id=\"jobDescriptionText\"]'))).get_attribute('innerText')\n",
        "                       #print(f'summary,{summary}')\n",
        "                    job_description.append(summary)\n",
        "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException):\n",
        "                    job_description.append('')\n",
        "\n",
        "\n",
        "                    #print(\"--------------\")\n",
        "                wd.switch_to.default_content()\n",
        "\n",
        "            except:\n",
        "                job_title.append('')\n",
        "                seniority.append('')\n",
        "                emp_type.append('')\n",
        "                industries.append('')\n",
        "                company_names.append('')\n",
        "                job_location.append('')\n",
        "                job_salary.append('')\n",
        "                job_qual.append('')\n",
        "                job_description.append('')   \n",
        "\n",
        "        return job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description\n",
        "    \n",
        "    ## Iterate over the different pages \n",
        "    i=1\n",
        "    count = 1\n",
        "\n",
        "    try:\n",
        "        wait.until(EC.element_to_be_clickable((By.ID,'popover-x'))).click()       \n",
        "    except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "        print(\"no pop up\")\n",
        "\n",
        "    job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description = extract(wait,job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description)\n",
        "    while i<= n_pages:\n",
        "        wd.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
        "        try:\n",
        "            #time.sleep(1)\n",
        "            button = wd.find_element(By.XPATH,f'//*[@id=\"resultsCol\"]/nav/div/ul/li[{i}]/a').click()\n",
        "            try:\n",
        "                wait.until(EC.element_to_be_clickable((By.ID,'popover-x'))).click()       \n",
        "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
        "                print(\"no pop up\")\n",
        "            job_id,job_title,seniority,emp_type,job_link,industries,company_names,job_date,job_location,job_salary,job_qual,job_description = extract(wait,job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description)\n",
        "            time.sleep(1)\n",
        "            #wd.back()\n",
        "            count += 1\n",
        "            print(f'COUNT = {count}')\n",
        "        except:\n",
        "            print(\"End of pages scaped\")\n",
        "        i+=1\n",
        "\n",
        "    ## Cleaning the dataframe \n",
        "    job_list = {'ID' : job_id,\n",
        "                'Title': job_title,\n",
        "                'Date': job_date, \n",
        "                'Company': company_names,\n",
        "                'Company2': company_names2,\n",
        "                'Location': job_location,\n",
        "                'Salary': job_salary,\n",
        "                #'Seniority': seniority,\n",
        "                'Employment Type': emp_type,\n",
        "                #'Function': job_func,\n",
        "                #'Industry': industries,\n",
        "                'Qualification': job_qual,\n",
        "                'Job description':job_description,\n",
        "                'Link': job_link}\n",
        "\n",
        "    df = pd.DataFrame(job_list)\n",
        "    df = df.replace('',np.nan)\n",
        "    df= df.dropna(subset=['Title'])\n",
        "    df['City'] = df['Location'].apply(lambda x: str(x).split(',')[0])\n",
        "    df['Zip Code'] = df['Location'].apply(lambda x: find_zip(str(x)))\n",
        "    df['Date Posted'] = df['Date'].apply(lambda x: convert_date(x))    \n",
        "    df['Date'] = df['Date'].apply(lambda x: split_date(x))\n",
        "\n",
        "    #df.to_csv(\"WebScrapper_Indeed\"+ str(time.time())+'.csv')\n",
        "\n",
        "    ## Homogenize the data frame:\n",
        "    df['Seniority'] = np.nan\n",
        "    df['Function'] = np.nan\n",
        "    df['Industry'] = np.nan\n",
        "    df['Source'] = 'Indeed'\n",
        "\n",
        "    cols = [ 'ID', 'Title', 'Date', 'Date Posted', 'Company', 'Company2','Location',\n",
        "                'Zip Code' ,'Salary','Employment Type', 'Qualification', 'Seniority', 'Function', 'Industry','Job description', 'Link', 'Source' ]\n",
        "    df= df[cols]\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78a45c34-84cd-4146-bc73-ccd395addf90",
      "metadata": {
        "id": "78a45c34-84cd-4146-bc73-ccd395addf90"
      },
      "source": [
        "## Monster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49cddcf0-57f5-40de-8e02-0a273a7ecc88",
      "metadata": {
        "id": "49cddcf0-57f5-40de-8e02-0a273a7ecc88"
      },
      "outputs": [],
      "source": [
        "def scrape_monster_jobs(keyword, location, n_pages):\n",
        "    url = f\"https://www.monster.com/jobs\"\n",
        "\n",
        "    ## Create an instance of the webdriver \n",
        "    s = Service('C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')\n",
        "    wd = webdriver.Chrome(service = s)\n",
        "\n",
        "    ## Open the Monster Webpage \n",
        "    wd.get(url)\n",
        "    wait = WebDriverWait(wd,10)\n",
        "\n",
        "    ## Input the Job title and location\n",
        "    what_search = wd.find_element(By.ID, \"search-job\" )\n",
        "    what_search.send_keys(keyword)\n",
        "\n",
        "    where_search = wd.find_element(By.ID, \"search-location\")\n",
        "    where_search.send_keys(location)\n",
        "    where_search.send_keys(Keys.RETURN)\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "    ## Execute script to scroll to bottom of the page \n",
        "    i=1\n",
        "    while (i <= n_pages):\n",
        "        wd.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
        "        #WebDriverWait(wd, 5).until(EC.element_to_be_clickable((By.XPATH,'/*[@id=\"__next\"]/div[3]/div[1]/main/div[2]/nav/section[1]/div[2]/div/div/div/div[64]/button'))).click()  \n",
        "        i+=1\n",
        "\n",
        "    wd.switch_to.window(wd.window_handles[0])\n",
        "    jobs = wd.find_element(By.XPATH,'//*[@id=\"__next\"]/div[3]/main/div[2]/nav/section[1]/div[2]/div/div/div').find_elements(By.CSS_SELECTOR,'div')\n",
        "    n_jobs = len(jobs)\n",
        "    print(n_jobs)\n",
        "\n",
        "    ## Initialise the different lists:\n",
        "    job_id =  []\n",
        "    job_title = []\n",
        "    seniority = []\n",
        "    emp_type=[]\n",
        "    job_link = []\n",
        "    industries = []\n",
        "    company_names = []\n",
        "    job_date =[]\n",
        "    job_location = []\n",
        "    job_salary = []\n",
        "    job_qual = []\n",
        "    job_description = []\n",
        "    \n",
        "    def scrape_jobs(n_jobs,job_id,job_title,seniority,emp_type,job_link,industries,company_names,job_date,job_location,job_salary,job_qual,job_description):\n",
        "        for i in range(1,n_jobs):         \n",
        "            try:\n",
        "                wd.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
        "                job = wd.find_element(By.XPATH,f' //*[@id=\"__next\"]/div[3]/main/div[2]/nav/section[1]/div[2]/div/div/div/div[{i}]/a/div/div[1]').click()      \n",
        "                wd.switch_to.window(wd.window_handles[1])\n",
        "                try:\n",
        "                    title = wd.find_element(By.TAG_NAME, \"h1\").text\n",
        "                    job_title.append(title)\n",
        "                    print(title)\n",
        "                except:\n",
        "                    job_title.append('')\n",
        "                try:\n",
        "                    date = wd.find_element(By.XPATH, \"//div[@data-test-id='svx-jobview-posted']\").text\n",
        "                    print(date)\n",
        "                            #date = wd.find_element(By.XPATH,'//*[@id=\"details-table\"]').find_element(By.CSS_SELECTOR, 'div.detailsstyles__DetailsTableDetailBody-sc-1deoovj-5.hDKBZO').text\n",
        "                    job_date.append(date)\n",
        "                except:\n",
        "                    job_date.append('')\n",
        "                try:\n",
        "                    company = wd.find_element(By.TAG_NAME, \"h2\").text\n",
        "                    company_names.append(company)\n",
        "                except:\n",
        "                    company_names.append('')\n",
        "                        #print(company)\n",
        "                try:\n",
        "                    location = wd.find_element(By.TAG_NAME, \"h3\").text\n",
        "                    job_location.append(location)\n",
        "                except:\n",
        "                    job_location.append('')\n",
        "                        #print(location)\n",
        "                try:\n",
        "                    link = wd.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
        "                    job_link.append(link)\n",
        "                except:\n",
        "                        job_link.append('')\n",
        "                        #print(link)\n",
        "                try:\n",
        "                    salary= wd.find_element(By.XPATH,'//*[@id=\"details-table\"]').find_element(By.CSS_SELECTOR,'div.salarystyle__SalaryContainer-sc-1kub5et-7.TQDXU > div').text\n",
        "                            #salary = wd.find_element(By.CLASS_NAME, \"salarystyle__SalaryBody-sc-1kub5et-8 jeLUTC\").text\n",
        "                    job_salary.append(salary)\n",
        "                except:\n",
        "                    job_salary.append('')\n",
        "\n",
        "                try:\n",
        "                           # emp = wd.find_element(By.XPATH, \"//div[@data-test-id='svx-jobview-location']\").text\n",
        "                    emp =wd.find_element(By.XPATH, \"//div[@data-test-id='svx-jobview-employmenttype']\").text\n",
        "                    emp_type.append(emp)\n",
        "                except:\n",
        "                    emp_type.append('')\n",
        "\n",
        "                try:\n",
        "                    description = wd.find_element(By.XPATH,'//*[@id=\"jobview-container\"]/div[1]/div/div[2]/div').text\n",
        "                    job_description.append(description)\n",
        "                        #print(description)\n",
        "                except:\n",
        "                    job_description.append('')\n",
        "\n",
        "                wd.close() \n",
        "                wd.switch_to.window(wd.window_handles[0]) \n",
        "                time.sleep(5)\n",
        "\n",
        "            except:\n",
        "                pass         #print(i)        \n",
        "        return  job_id,job_title,seniority,emp_type,job_link,industries,company_names,job_date,job_location,job_salary,job_qual,job_description\n",
        "\n",
        "    job_id,job_title,seniority,emp_type,job_link,industries,company_names,job_date,job_location,job_salary,job_qual,job_description=scrape_jobs(n_jobs,job_id,job_title,seniority,emp_type,job_link,industries,company_names,job_date,job_location,job_salary,job_qual,job_description)\n",
        "\n",
        "    ## Clean out the dataframe \n",
        "    job_list = {#'ID' : job_id,\n",
        "                'Title': job_title,\n",
        "                'Date': job_date, \n",
        "                'Company': company_names,\n",
        "                'Location': job_location,\n",
        "                'Salary': job_salary,\n",
        "                #'Seniority': seniority,\n",
        "                'Employment Type': emp_type,\n",
        "                #'Function': job_func,\n",
        "                #'Industry': industries,\n",
        "                #'Qualification': job_qual,\n",
        "                'Job description':job_description,\n",
        "                'Link': job_link}\n",
        "    df = pd.DataFrame(job_list)\n",
        "\n",
        "    ## Remove empty lines \n",
        "    df = df.replace('',np.nan)\n",
        "    df= df.dropna(subset=['Title'])\n",
        "\n",
        "    # Convert the date\n",
        "    df['Date Posted'] = df['Date'].apply(lambda x: convert_date(x))\n",
        "    df['Date'] = df['Date'].apply(lambda x: split_date(x))\n",
        "\n",
        "    ## Add variables that don't exsist in the website \n",
        "    df['ID'] = np.nan\n",
        "    df['Function'] = np.nan\n",
        "    df['Industry']= np.nan\n",
        "    df['Qualification'] = np.nan\n",
        "    df['Seniority'] = np.nan\n",
        "    df['Industry'] = np.nan\n",
        "    df['Zip Code'] = np.nan\n",
        "    df['Company2'] = np.nan\n",
        "    df['Source'] = 'Monster'\n",
        "    \n",
        "    cols = [ 'ID', 'Title', 'Date', 'Date Posted', 'Company', 'Company2','Location',\n",
        "            'Zip Code' ,'Salary','Employment Type', 'Qualification', 'Seniority', 'Function', 'Industry','Job description', 'Link','Source' ]\n",
        "    df= df[cols]\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d765e7e1-d512-4646-a98f-480bf2a46731",
      "metadata": {
        "id": "d765e7e1-d512-4646-a98f-480bf2a46731"
      },
      "source": [
        "## Linkedin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6c12c89-74f2-432e-8a22-a2add7f152e8",
      "metadata": {
        "id": "e6c12c89-74f2-432e-8a22-a2add7f152e8"
      },
      "outputs": [],
      "source": [
        "def scrape_linkedin_jobs(keyword, location, n_pages):\n",
        "\n",
        "    s = Service ('C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')\n",
        "    wd = webdriver.Chrome(service = s)\n",
        "    wait = WebDriverWait(wd, 1)\n",
        "    actions = ActionChains(wd)\n",
        "    ## change User Agent at each request \n",
        "\n",
        "    username = 'contact@autmhq.org'\n",
        "    password = \"2023GlobalTech\"\n",
        "    url = \"https://www.linkedin.com/\"\n",
        "    wd.get(url)\n",
        "\n",
        "\n",
        "    ## Sign_in in LinkedIn \n",
        "    signin = wd.find_element(By.XPATH, '/html/body/nav/div/a[2]').click()\n",
        "    ids = wd.find_element(By.ID, \"username\")\n",
        "    ids.send_keys (username)\n",
        "    code =  wd.find_element(By.ID, \"password\") \n",
        "    code.send_keys(password)\n",
        "    code.send_keys(Keys.RETURN)\n",
        "\n",
        "\n",
        "    wd.find_element(By.XPATH,'//*[@id=\"ember20\"]').click()\n",
        "    time.sleep(3)\n",
        "    what_search =  wd.find_element(By.CSS_SELECTOR,(\"input[id*='jobs-search-box-keyword-id']\"))\n",
        "    what_search.send_keys(keyword)\n",
        "    time.sleep(2)\n",
        "    where_search =  wd.find_element(By.CSS_SELECTOR,(\"input[id*='jobs-search-box-location-id']\"))\n",
        "    time.sleep(2)\n",
        "    where_search.send_keys(place)\n",
        "    what_search.send_keys(Keys.RETURN)\n",
        "\n",
        "    ## Iterate over the number of pages:\n",
        "    page = 1 \n",
        "        #while i <= int(no_of_jobs/len(jobs_per_page))+1:\n",
        "    while page <= n_pages:\n",
        "        wd.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
        "        page = page+1\n",
        "        try: \n",
        "            wd.find_element(By.XPATH,'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/section/div/ul/li[{page}]').click()\n",
        "            time.sleep(5)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        time.sleep(5)\n",
        "\n",
        "    wd.maximize_window()\n",
        "     ## Find the number of jobs on one page:\n",
        "    job_list = wd.find_element(By.CLASS_NAME,'jobs-search-results__list')\n",
        "    jobs = job_list.find_elements(By.CLASS_NAME,'jobs-search-results__list-item')\n",
        "    n_jobs = len(jobs)\n",
        "    #print(n_jobs)\n",
        "    init_url= wd.current_url\n",
        "    #print(init_url)\n",
        "\n",
        "    ## Initialize the different lists\n",
        "    job_id = []\n",
        "    job_title = []\n",
        "    company = []\n",
        "    job_location = []\n",
        "    job_date = []\n",
        "    job_link = []\n",
        "    job_description = []\n",
        "    seniority = []\n",
        "    emp_type=[]\n",
        "    job_func = []\n",
        "    industries = []\n",
        "    company_names = []\n",
        "    \n",
        "    i= 1\n",
        "    for job in jobs:\n",
        "        try:\n",
        "            ids = job.get_attribute(\"data-job-id\")\n",
        "            job_id.append(ids)\n",
        "        except:\n",
        "            job_id.append('')\n",
        "            print('id fail')\n",
        "        try:\n",
        "            title = wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[1]/a'))).text\n",
        "            job_title.append(title)\n",
        "        except:\n",
        "            job_title.append('')\n",
        "            print('title fail')\n",
        "        try:\n",
        "            comp= wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[2]/a'))).text\n",
        "            company.append(comp)\n",
        "\n",
        "        except:\n",
        "            company.append('')\n",
        "            print('company fail')\n",
        "        try:\n",
        "            location = wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[3]/ul/li'))).text  \n",
        "            job_location.append(location)\n",
        "\n",
        "        except:\n",
        "            job_location.append('')\n",
        "            print('location fail')\n",
        "\n",
        "        try:\n",
        "            uid = job.get_attribute(\"id\")\n",
        "            path = r'//*[@id = \"{}\"]/div/div'.format(uid)\n",
        "            #print(uid,path)\n",
        "            wd.find_element(By.XPATH,path).click()\n",
        "            cur_url = wd.current_url\n",
        "\n",
        "            try: \n",
        "                link = wait.until(EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/a'))).get_attribute('href')\n",
        "                print(link)\n",
        "                job_link.append(link)\n",
        "            except:\n",
        "                job_link.append('')\n",
        "\n",
        "            try:\n",
        "                emp = wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/div[2]/ul/li[1]/span'))).text\n",
        "                print(emp)\n",
        "                emp_type.append(emp)\n",
        "               # print(\"Emp type  \",emp_type)\n",
        "            except:\n",
        "                emp_type.apppend('')\n",
        "\n",
        "            try:\n",
        "                industry = wait.until(EC.presence_of_element_located((By.XPATH,'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/div[2]/ul/li[2]/span/a'))).text\n",
        "                industries.append(industry)\n",
        "              #  print(\"Industry   \",industry)\n",
        "            except:\n",
        "                industries.append('')\n",
        "\n",
        "            try:\n",
        "                age = wait.until(EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/div[1]/span[2]/span[1]'))).text\n",
        "                job_date.append(age)\n",
        "            except:\n",
        "                job_date.append('')\n",
        "\n",
        "            try:\n",
        "                description = wait.until(EC.presence_of_element_located((By.XPATH,'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[2]/article/div/div[1]/span'))).text\n",
        "                job_description.append(description)\n",
        "            except:\n",
        "                job_description.append('')\n",
        "\n",
        "            #if init_url != cur_url:\n",
        "             #   wd.back()\n",
        "\n",
        "\n",
        "        except:\n",
        "            print('fail')\n",
        "            job_link.append('')\n",
        "            job_date.append('')\n",
        "            job_description.append('')\n",
        "            emp_type.append('')\n",
        "            industries.append('')\n",
        "\n",
        "       # if init_url != cur_url:\n",
        "        #    wd.back()\n",
        "\n",
        "\n",
        "\n",
        "        i= i+1\n",
        "        print(i)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({ 'ID' : job_id,\n",
        "                             'Title': job_title,\n",
        "                             'Date Posted': job_date, \n",
        "                             'Company': company,\n",
        "                             'Location': job_location,\n",
        "                             'Employment Type': emp_type,\n",
        "                             'Industry': industries,\n",
        "                             'Link': job_link,\n",
        "                          #   'Job description': job_description\n",
        "                        })\n",
        "\n",
        "    df['Company2'] = np.nan\n",
        "    df['Zip Code'] = np.nan\n",
        "    df['Date'] = np.nan\n",
        "    df['Qualification'] = np.nan\n",
        "    df['Salary'] = np.nan\n",
        "    df['Source'] = 'LinkedIn'\n",
        "\n",
        "    cols = [ 'ID', 'Title', 'Date', 'Date Posted', 'Company', 'Company2','Location',\n",
        "                'Zip Code' ,'Salary','Employment Type', 'Qualification', 'Industry','Job description', 'Link','Source']\n",
        "    #df= df[cols]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4dfed80-9c8d-454a-aebf-b72648503f8f",
      "metadata": {
        "id": "d4dfed80-9c8d-454a-aebf-b72648503f8f"
      },
      "source": [
        "## TODO \n",
        "For each data frame scrapped:\n",
        " <ul>\n",
        "  <li> Write a script that scrapes jobs every day </li>\n",
        "  <li> Check that there is no duplicate data  </li>\n",
        "  <li> Copy the cleaned data and export to our master list  </li>\n",
        "  <li> Check that there's no duplicate data when we export the new jobs  </li>\n",
        "  <li> Remove jobs that are too old </li>\n",
        "  <li> Scrape jobs: just get the first page for each job</li>\n",
        " </ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8e8538-64d5-4f21-b86a-ce15cbef1519",
      "metadata": {
        "id": "3d8e8538-64d5-4f21-b86a-ce15cbef1519"
      },
      "outputs": [],
      "source": [
        "## Input the keywords \n",
        "#keyword = input(\"Enter the postions you are scraping\")\n",
        "#place = input (\"Enter the job's location \")\n",
        "keyword = \"Software Engineer\"\n",
        "place = \"Austin TEXAS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a1b437-b026-47b2-afa2-6290fcfabdf3",
      "metadata": {
        "id": "22a1b437-b026-47b2-afa2-6290fcfabdf3"
      },
      "outputs": [],
      "source": [
        "def update_master_df():\n",
        "    master_df = pd.read_csv(\"Master_job_list.csv\")\n",
        "    \n",
        "    # Remove duplicates\n",
        "    master_df.drop_duplicates(inplace = True)\n",
        "    \n",
        "    # Remove jobs that are too old (posted more than 2 weeks ago)\n",
        "   # date = \n",
        "    master_df = master_df[(master_df['Date Posted'] > (date.today() -timedelta(14)).strftime('%m/%d/%Y')) & (master_df['Date Posted'] <=  (date.today()+timedelta(14)).strftime('%m/%d/%Y'))]\n",
        "    \n",
        "    # Save master_df\n",
        "    master_df.to_csv(\"Master_job_list.csv\",index = False)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d43f6f-ff95-4503-b140-c90867ab6828",
      "metadata": {
        "id": "96d43f6f-ff95-4503-b140-c90867ab6828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "aec00757-7ae5-47a7-8711-33ad687cbe5c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "WebDriverException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                             \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                             creationflags=self.creationflags)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1550\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Program Files (x86)\\\\chromedriver_win32\\\\chromedriver.exe': 'C:\\\\Program Files (x86)\\\\chromedriver_win32\\\\chromedriver.exe'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1f0890ce607f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## run scripts to scrape data from LinkedIn, Monster, Indeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlinkedin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_linkedin_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmonster_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mscrape_monster_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindeed_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mscrape_indeed_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6ca88587876c>\u001b[0m in \u001b[0;36mscrape_linkedin_jobs\u001b[0;34m(keyword, location, n_pages)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mService\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActionChains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[1;32m     71\u001b[0m                                         \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                         \u001b[0mservice_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_capabilities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                                         service_log_path, service, keep_alive)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 raise WebDriverException(\n\u001b[1;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m---> 83\u001b[0;31m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[1;32m     84\u001b[0m                 )\n\u001b[1;32m     85\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebDriverException\u001b[0m: Message: 'C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe' executable needs to be in PATH. Please see https://chromedriver.chromium.org/home\n"
          ]
        }
      ],
      "source": [
        "## run scripts to scrape data from LinkedIn, Monster, Indeed \n",
        "while(True):\n",
        "    linkedin_df = scrape_linkedin_jobs(keyword, place ,1)\n",
        "    monster_df= scrape_monster_jobs(keyword, place , 1)\n",
        "    indeed_df= scrape_indeed_jobs(keyword, place , 1)\n",
        "    make_noise()\n",
        "    \n",
        "    ## Merge scrapped data in one file \n",
        "    df = pd.concat([linkedin_df,monster_df,indeed_df])\n",
        "  \n",
        "    ## Remove the duplicates from the total dataframe \n",
        "    df.drop_duplicates(inplace = True)\n",
        "\n",
        "    ## Export cleaned data frame to our master list \n",
        "    #df.to_csv(\"Master_job_list.csv\", index = False, header = True) ## To be used for first run \n",
        "    df.to_csv(\"Master_job_list.csv\", mode = 'a', index = False, header = False) \n",
        "\n",
        "    ## Remove jobs that are too old: \n",
        "    update_master_df()\n",
        "    #time.sleep(86400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0873cc-a242-4b42-ae9a-7e154b2dc3be",
      "metadata": {
        "id": "ea0873cc-a242-4b42-ae9a-7e154b2dc3be"
      },
      "outputs": [],
      "source": [
        "    ## Merge scrapped data in one file \n",
        "df = pd.concat([linkedin_df,monster_df,indeed_df])\n",
        "   # df = pd.concat([monster_df,indeed_df])\n",
        "    ## Remove the duplicates from the total dataframe \n",
        "df.drop_duplicates(inplace = True)\n",
        "\n",
        "    ## Export cleaned data frame to our master list #df.to_csv(\"Master_job_list.csv\", index = False, header = True) \n",
        "df.to_csv(\"Master_job_list.csv\", mode = 'a', index = False, header = False) \n",
        "\n",
        "    ## Remove jobs that are too old: \n",
        "    #df\n",
        "update_master_df()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc484c0-ad6e-4c26-b449-5470c206c0b9",
      "metadata": {
        "id": "5cc484c0-ad6e-4c26-b449-5470c206c0b9"
      },
      "outputs": [],
      "source": [
        "## Next Steps \n",
        "Extract salary, Education level from the job description"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "automated_job_scraper.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}